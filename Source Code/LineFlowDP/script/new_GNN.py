import torch
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing
from torch_geometric.data import Data
from torch_geometric.utils import to_networkx
import networkx as nx
from tqdm import tqdm
import os
from train_rgcn import RGCN  # â¬…ï¸ ç¡®ä¿ä½ çš„ `train_rgcn.py` é‡Œå®šä¹‰äº† RGCN ç±»
import argparse

EPS = 1e-15  # é¿å…æ•°å€¼è®¡ç®—é”™è¯¯


class GNNExplainer(torch.nn.Module):
    def __init__(self, model, epochs=100, lr=0.01, log=True):
        super(GNNExplainer, self).__init__()
        self.model = model
        self.epochs = epochs
        self.lr = lr
        self.log = log

        # æ­£åˆ™åŒ–å‚æ•°
        self.coeffs = {
            'edge_size': 0.005,
            'node_feat_size': 1.0,
            'edge_ent': 1.0,
            'node_feat_ent': 0.1,
        }

    def set_masks(self, x, edge_index):
        """ åˆå§‹åŒ–èŠ‚ç‚¹ç‰¹å¾å’Œè¾¹æ©ç  """
        N, F = x.shape  # èŠ‚ç‚¹æ•°, ç‰¹å¾ç»´åº¦
        E = edge_index.size(1)  # è¾¹æ•°

        self.node_feat_mask = torch.nn.Parameter(torch.randn(N, F) * 0.1)
        self.edge_mask = torch.nn.Parameter(torch.randn(E) * 0.1)

        # è®¾ç½® GNN è§£é‡Šæ¨¡å¼
        for module in self.model.modules():
            if isinstance(module, MessagePassing):
                module.__explain__ = True
                module.__edge_mask__ = self.edge_mask

    def clear_masks(self):
        """ æ¸…é™¤æ©ç  """
        for module in self.model.modules():
            if isinstance(module, MessagePassing):
                module.__explain__ = False
                module.__edge_mask__ = None
        self.node_feat_mask = None
        self.edge_mask = None

    def loss(self, log_logits, pred_label):
        """ è®¡ç®—æŸå¤± """
        loss = -log_logits[0, pred_label[0]]  # è®¡ç®—å›¾çº§æŸå¤±
        loss += self.coeffs['edge_size'] * self.edge_mask.sigmoid().sum()
        loss += self.coeffs['node_feat_size'] * self.node_feat_mask.sigmoid().sum()
        return loss

    def explain_graph(self, data):
        """ è§£é‡Šæ•´ä¸ªå›¾ï¼Œè¿”å›èŠ‚ç‚¹é‡è¦æ€§å’Œè¾¹é‡è¦æ€§ """
        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr
        self.model.eval()
        self.clear_masks()

        # å¤„ç† edge_attr ä½œä¸º edge_type
        edge_type = edge_attr.argmax(dim=1) if edge_attr.dim() > 1 else edge_attr

        with torch.no_grad():
            out = self.model(x, edge_index, edge_type, data.batch)
            log_logits = F.log_softmax(out, dim=1)
            pred_label = log_logits.argmax(dim=-1)

        self.set_masks(x, edge_index)
        self.to(x.device)

        optimizer = torch.optim.Adam([self.node_feat_mask, self.edge_mask], lr=self.lr)

        if self.log:
            pbar = tqdm(total=self.epochs, desc="Explaining graph")

        for epoch in range(1, self.epochs + 1):
            optimizer.zero_grad()
            h = x * self.node_feat_mask.sigmoid()
            out = self.model(h, edge_index, edge_type, data.batch)
            log_logits = F.log_softmax(out, dim=1)
            loss = self.loss(log_logits, pred_label)
            loss.backward()
            optimizer.step()

            if self.log:
                pbar.update(1)

        if self.log:
            pbar.close()

        return self.node_feat_mask.detach().sigmoid(), self.edge_mask.detach().sigmoid()

def recall_at_top_20_loc(risk_scores, defective_lines, total_lines):
    """
    è®¡ç®— Recall@Top20%LOC
    :param risk_scores: {è¡Œå·: é£é™©è¯„åˆ†}ï¼ŒæŒ‰è¯„åˆ†é™åºæ’åˆ—
    :param defective_lines: çœŸå®ç¼ºé™·è¡Œçš„é›†åˆ {è¡Œå·1, è¡Œå·2, ...}
    :param total_lines: æ€»ä»£ç è¡Œæ•°
    :return: Recall@Top20%LOC
    """
    top_20_loc = int(total_lines * 0.2)  # è®¡ç®—å‰ 20% ä»£ç è¡Œæ•°
    sorted_lines = sorted(risk_scores, key=risk_scores.get, reverse=True)  # æŒ‰é£é™©é™åºæ’åº
    selected_lines = set(sorted_lines[:top_20_loc])  # å–å‰ 20% è¡Œ

    found_defective = len(selected_lines & defective_lines)  # è®¡ç®—æ‰¾åˆ°çš„ç¼ºé™·è¡Œæ•°
    total_defective = len(defective_lines)  # çœŸå®çš„ç¼ºé™·è¡Œæ•°

    return found_defective / total_defective if total_defective > 0 else 0.0


def effort_at_top_20_recall(risk_scores, defective_lines, total_lines):
    """
    è®¡ç®— Effort@Top20%Recall
    :param risk_scores: {è¡Œå·: é£é™©è¯„åˆ†}ï¼ŒæŒ‰è¯„åˆ†é™åºæ’åˆ—
    :param defective_lines: çœŸå®ç¼ºé™·è¡Œçš„é›†åˆ {è¡Œå·1, è¡Œå·2, ...}
    :param total_lines: æ€»ä»£ç è¡Œæ•°
    :return: Effort@Top20%Recall
    """
    total_defective = len(defective_lines)
    top_20_defective = int(total_defective * 0.2)  # éœ€è¦æ‰¾åˆ°çš„ç¼ºé™·è¡Œæ•°
    sorted_lines = sorted(risk_scores, key=risk_scores.get, reverse=True)  # æŒ‰é£é™©é™åºæ’åº

    found_defective = 0
    effort_LOC = 0

    for line in sorted_lines:
        effort_LOC += 1
        if line in defective_lines:
            found_defective += 1
        if found_defective >= top_20_defective:
            break

    return effort_LOC / total_lines if total_lines > 0 else 0.0


def initial_false_alarms(risk_scores, defective_lines):
    """
    è®¡ç®— IFAï¼ˆInitial False Alarmsï¼‰
    :param risk_scores: {è¡Œå·: é£é™©è¯„åˆ†}ï¼ŒæŒ‰è¯„åˆ†é™åºæ’åˆ—
    :param defective_lines: çœŸå®ç¼ºé™·è¡Œçš„é›†åˆ {è¡Œå·1, è¡Œå·2, ...}
    :return: IFA å€¼
    """
    sorted_lines = sorted(risk_scores, key=risk_scores.get, reverse=True)  # æŒ‰é£é™©é™åºæ’åº

    false_alarms = 0
    for line in sorted_lines:
        if line in defective_lines:
            break  # å‘ç°ç¬¬ä¸€ä¸ªç¼ºé™·è¡Œï¼Œåœæ­¢è®¡æ•°
        false_alarms += 1

    return false_alarms

def top_k_accuracy(risk_scores, defective_lines, k):
    """
    è®¡ç®— Top-k Accuracy
    :param risk_scores: {è¡Œå·: é£é™©è¯„åˆ†}ï¼ŒæŒ‰è¯„åˆ†é™åºæ’åˆ—
    :param defective_lines: çœŸå®ç¼ºé™·è¡Œçš„é›†åˆ {è¡Œå·1, è¡Œå·2, ...}
    :param k: é€‰å–å‰ k è¡Œ
    :return: 1ï¼ˆåŒ…å«ç¼ºé™·ï¼‰æˆ– 0ï¼ˆä¸åŒ…å«ç¼ºé™·ï¼‰
    """
    sorted_lines = sorted(risk_scores, key=risk_scores.get, reverse=True)[:k]  # å–å‰ k è¡Œ
    return 1 if any(line in defective_lines for line in sorted_lines) else 0



# ----------------- åŠ è½½æ¨¡å‹å’Œæ•°æ® -----------------
def load_model(model_path, num_features, num_classes, num_relations, device):
    """ åŠ è½½è®­ç»ƒå¥½çš„ RGCN æ¨¡å‹ """
    model = RGCN(num_features, 128, num_classes, num_relations).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    return model


def load_data(data_path):
    """ åŠ è½½æ•°æ®é›† """
    data_list = torch.load(data_path)
    return data_list

def analyze_defective_file(model, data):
    """ è¿è¡Œå®Œæ•´æµç¨‹ï¼šä½¿ç”¨ GNNExplainer è§£é‡Šé¢„æµ‹ç»“æœï¼Œå¹¶è®¡ç®— SNA é£é™© """
    explainer = GNNExplainer(model)
    node_mask, edge_mask = explainer.explain_graph(data)

    # è®¡ç®—é£é™©åˆ†æ•°
    risk_scores = {i: node_mask[i].mean().item() for i in range(len(node_mask))}

    # è·å–çœŸå®ç¼ºé™·è¡Œ
    defective_lines = {i for i, label in enumerate(data.node_label) if label == 1}
    total_lines = len(data.node_label)

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    recall = recall_at_top_20_loc(risk_scores, defective_lines, total_lines)
    effort = effort_at_top_20_recall(risk_scores, defective_lines, total_lines)
    ifa = initial_false_alarms(risk_scores, defective_lines)
    top_k_acc = top_k_accuracy(risk_scores, defective_lines, k=10)

    # è¾“å‡ºæ’å
    sorted_risks = sorted(risk_scores.items(), key=lambda x: x[1], reverse=True)
    print("\nğŸ” ä»£ç è¡Œé£é™©æ’åï¼ˆå‰ 10 ä¸ªï¼‰ï¼š")
    for node, score in sorted_risks[:10]:
        print(f"è¡Œ {node}: é£é™©è¯„åˆ† {score:.4f}")

    # è¾“å‡ºè¯„ä¼°ç»“æœ
    print(f"\nğŸ“Š Recall@Top20%LOC: {recall:.4f}")
    print(f"ğŸ“Š Effort@Top20%Recall: {effort:.4f}")
    print(f"ğŸ“Š Initial False Alarms: {ifa}")
    print(f"ğŸ“Š Top-10 Accuracy: {top_k_acc}")

    return sorted_risks

# ----------------- ä¸»å‡½æ•° -----------------
def main():
    DATA_PATH = "./data/activemq/activemq-5.0.0/processed/data.pt"
    MODEL_PATH = "rgcn_model.pth"
    parser = argparse.ArgumentParser(description="Run GNNExplainer on RGCN model.")
    parser.add_argument("--data", type=str, default="./data/activemq/activemq-5.0.0/processed/data.pt", help="Path to data.pt")
    parser.add_argument("--model", type=str, default="rgcn_model.pth", help="Path to trained model")
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åŠ è½½æ•°æ®
    data_list = load_data(DATA_PATH)
    print(f"âœ… åŠ è½½æ•°æ®æˆåŠŸï¼Œå‘ç° {len(data_list)} ä¸ªå›¾")

    # é€‰æ‹©ç¬¬ä¸€ä¸ª PDG è¿›è¡Œåˆ†æ
    data = data_list[0].to(device)

    # è·å–æ¨¡å‹å‚æ•°
    num_features = data.x.shape[1]
    num_classes = 2  # å‡è®¾äºŒåˆ†ç±»
    num_relations = data.edge_attr.size(1) if data.edge_attr.dim() > 1 else 1

    # åŠ è½½æ¨¡å‹
    model = load_model(MODEL_PATH, num_features, num_classes, num_relations, device)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

    # éå†æ‰€æœ‰ PDG å¹¶åˆ†æ
    # for idx, data in enumerate(data_list):
    #     data = data.to(device)
    #     print(f"\n åˆ†æç¬¬ {idx + 1} ä¸ªå›¾...")
    #     analyze_defective_file(model, data)
    data = data_list[2].to(device)
    analyze_defective_file(model, data)
    defective_lines = {i for i, label in enumerate(data.y) if label == 1}
    print(f"ğŸ“Œ `y` å”¯ä¸€å€¼: {torch.unique(data.y) if hasattr(data, 'y') else 'æ—  y'}")
    print(
        f"ğŸ“Œ `graph_labels` å”¯ä¸€å€¼: {torch.unique(data.graph_labels) if hasattr(data, 'graph_labels') else 'æ—  graph_labels'}")

    print(f"ğŸ“Œ çœŸå®ç¼ºé™·è¡Œ: {defective_lines}")
    print(f"ğŸ“Œ `node_label` å”¯ä¸€å€¼: {torch.unique(data.node_label)}")


if __name__ == "__main__":
    main()
