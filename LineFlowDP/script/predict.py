import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import torch
import torch.nn.functional as F
import networkx as nx
from torch_geometric.nn import RGCNConv, global_mean_pool
from torch_geometric.utils import to_networkx, k_hop_subgraph
from tqdm import tqdm
from math import sqrt

from train_rgcn import RGCN, all_data


EPS = 1e-15  # é¿å…æ•°å€¼è®¡ç®—é”™è¯¯


class GNNExplainer(torch.nn.Module):
    def __init__(self, model, epochs=100, lr=0.01, num_hops=None, return_type='log_prob', log=True):
        super(GNNExplainer, self).__init__()
        self.model = model
        self.epochs = epochs
        self.lr = lr
        self.num_hops = num_hops
        self.return_type = return_type
        self.log = log

    def set_masks(self, x, edge_index, edge_attr):
        """ åˆå§‹åŒ–èŠ‚ç‚¹ç‰¹å¾å’Œè¾¹æ©ç  """
        F, E = x.size(1), edge_index.size(1)
        self.node_feat_mask = torch.nn.Parameter(torch.randn(F) * 0.1)
        self.edge_mask = torch.nn.Parameter(torch.randn(E) * 0.1)

        for module in self.model.modules():
            if isinstance(module, RGCNConv):
                module.__explain__ = True
                module.__edge_mask__ = self.edge_mask

    def clear_masks(self):
        """ æ¸…é™¤æ©ç ï¼Œæ¢å¤æ¨¡å‹çŠ¶æ€ """
        for module in self.model.modules():
            if isinstance(module, RGCNConv):
                module.__explain__ = False
                module.__edge_mask__ = None
        self.node_feat_mask = None
        self.edge_mask = None

    def loss(self, log_logits, pred_label):
        """ è®¡ç®—æŸå¤±ï¼Œä¼˜åŒ–æ©ç  """
        loss = -log_logits[0, pred_label[0]]
        m = self.edge_mask.sigmoid()
        loss += 0.005 * m.sum()  # æ§åˆ¶è¾¹æ©ç å¤§å°
        loss += 1.0 * (-m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)).mean()  # ç†µæ­£åˆ™åŒ–
        return loss

    def explain_graph(self, data):
        """ è§£é‡Šæ•´ä¸ªå›¾ï¼Œè¿”å›èŠ‚ç‚¹é‡è¦æ€§å’Œè¾¹é‡è¦æ€§ """
        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr
        self.model.eval()
        self.clear_masks()

        with torch.no_grad():
            out = self.model(x, edge_index, edge_attr.argmax(dim=1), data.batch)
            log_logits = F.log_softmax(out, dim=1)
            pred_label = log_logits.argmax(dim=-1)

        self.set_masks(x, edge_index, edge_attr)
        self.to(x.device)
        optimizer = torch.optim.Adam([self.node_feat_mask, self.edge_mask], lr=self.lr)

        if self.log:
            pbar = tqdm(total=self.epochs, desc="Explaining graph")

        for epoch in range(1, self.epochs + 1):
            optimizer.zero_grad()
            h = x * self.node_feat_mask.sigmoid()
            out = self.model(h, edge_index, edge_attr.argmax(dim=1), data.batch)
            log_logits = F.log_softmax(out, dim=1)
            loss = self.loss(log_logits, pred_label)
            loss.backward()
            optimizer.step()

            if self.log:
                pbar.update(1)

        if self.log:
            pbar.close()

        return self.node_feat_mask.detach().sigmoid(), self.edge_mask.detach().sigmoid()


# ----------------- è®¡ç®—ä»£ç è¡Œé£é™©è¯„åˆ† -----------------
def compute_sna_risk(data, edge_mask):
    """ è®¡ç®— SNA æŒ‡æ ‡ï¼šåº¦ä¸­å¿ƒæ€§ã€Katz ä¸­å¿ƒæ€§ã€æ¥è¿‘ä¸­å¿ƒæ€§ """
    G = to_networkx(data, to_undirected=True)

    # è®¡ç®— Degree Centrality
    degree_centrality = nx.degree_centrality(G)

    # è®¡ç®— Katz Centrality
    try:
        katz_centrality = nx.katz_centrality_numpy(G, alpha=0.01, beta=1.0)
    except nx.NetworkXError:
        katz_centrality = {n: 0 for n in G.nodes}

    # è®¡ç®— Closeness Centrality
    closeness_centrality = nx.closeness_centrality(G)

    # å½’ä¸€åŒ–
    max_degree = max(degree_centrality.values()) if degree_centrality else 1
    max_katz = max(katz_centrality.values()) if katz_centrality else 1
    max_closeness = max(closeness_centrality.values()) if closeness_centrality else 1

    risk_scores = {}
    for node in G.nodes:
        risk_scores[node] = (
                (degree_centrality[node] / max_degree) +
                (katz_centrality[node] / max_katz) +
                (closeness_centrality[node] / max_closeness)
        )

    return risk_scores


# ----------------- è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹ -----------------
def analyze_defective_file(model, data):
    """ è¿è¡Œå®Œæ•´æµç¨‹ï¼šä½¿ç”¨ GNNExplainer è§£é‡Šé¢„æµ‹ç»“æœï¼Œå¹¶è®¡ç®— SNA é£é™© """
    explainer = GNNExplainer(model)
    node_mask, edge_mask = explainer.explain_graph(data)

    # è®¡ç®— SNA é£é™©
    risk_scores = compute_sna_risk(data, edge_mask)

    # æ’åºï¼Œè¾“å‡ºé£é™©æœ€é«˜çš„ä»£ç è¡Œ
    sorted_risks = sorted(risk_scores.items(), key=lambda x: x[1], reverse=True)
    print("\nğŸ” ä»£ç è¡Œé£é™©æ’åï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š")
    for node, score in sorted_risks[:10]:  # åªæ˜¾ç¤ºå‰ 10 è¡Œ
        print(f"è¡Œ {node}: é£é™©è¯„åˆ† {score:.4f}")

    return sorted_risks


def recall_at_top_20_loc(risk_scores, defective_lines, total_lines):
    """
    è®¡ç®— Recall@Top20%LOC
    :param risk_scores: {è¡Œå·: é£é™©è¯„åˆ†}ï¼ŒæŒ‰è¯„åˆ†é™åºæ’åˆ—
    :param defective_lines: çœŸå®ç¼ºé™·è¡Œçš„é›†åˆ {è¡Œå·1, è¡Œå·2, ...}
    :param total_lines: æ€»ä»£ç è¡Œæ•°
    :return: Recall@Top20%LOC
    """
    top_20_loc = int(total_lines * 0.2)  # è®¡ç®—å‰ 20% ä»£ç è¡Œæ•°
    sorted_lines = sorted(risk_scores, key=risk_scores.get, reverse=True)  # æŒ‰é£é™©é™åºæ’åº
    selected_lines = set(sorted_lines[:top_20_loc])  # å–å‰ 20% è¡Œ

    found_defective = len(selected_lines & defective_lines)  # è®¡ç®—æ‰¾åˆ°çš„ç¼ºé™·è¡Œæ•°
    total_defective = len(defective_lines)  # çœŸå®çš„ç¼ºé™·è¡Œæ•°

    return found_defective / total_defective if total_defective > 0 else 0.0


def effort_at_top_20_recall(risk_scores, defective_lines, total_lines):
    """
    è®¡ç®— Effort@Top20%Recall
    :param risk_scores: {è¡Œå·: é£é™©è¯„åˆ†}ï¼ŒæŒ‰è¯„åˆ†é™åºæ’åˆ—
    :param defective_lines: çœŸå®ç¼ºé™·è¡Œçš„é›†åˆ {è¡Œå·1, è¡Œå·2, ...}
    :param total_lines: æ€»ä»£ç è¡Œæ•°
    :return: Effort@Top20%Recall
    """
    total_defective = len(defective_lines)
    top_20_defective = int(total_defective * 0.2)  # éœ€è¦æ‰¾åˆ°çš„ç¼ºé™·è¡Œæ•°
    sorted_lines = sorted(risk_scores, key=risk_scores.get, reverse=True)  # æŒ‰é£é™©é™åºæ’åº

    found_defective = 0
    effort_LOC = 0

    for line in sorted_lines:
        effort_LOC += 1
        if line in defective_lines:
            found_defective += 1
        if found_defective >= top_20_defective:
            break

    return effort_LOC / total_lines if total_lines > 0 else 0.0


def initial_false_alarms(risk_scores, defective_lines):
    """
    è®¡ç®— IFAï¼ˆInitial False Alarmsï¼‰
    :param risk_scores: {è¡Œå·: é£é™©è¯„åˆ†}ï¼ŒæŒ‰è¯„åˆ†é™åºæ’åˆ—
    :param defective_lines: çœŸå®ç¼ºé™·è¡Œçš„é›†åˆ {è¡Œå·1, è¡Œå·2, ...}
    :return: IFA å€¼
    """
    sorted_lines = sorted(risk_scores, key=risk_scores.get, reverse=True)  # æŒ‰é£é™©é™åºæ’åº

    false_alarms = 0
    for line in sorted_lines:
        if line in defective_lines:
            break  # å‘ç°ç¬¬ä¸€ä¸ªç¼ºé™·è¡Œï¼Œåœæ­¢è®¡æ•°
        false_alarms += 1

    return false_alarms

def top_k_accuracy(risk_scores, defective_lines, k):
    """
    è®¡ç®— Top-k Accuracy
    :param risk_scores: {è¡Œå·: é£é™©è¯„åˆ†}ï¼ŒæŒ‰è¯„åˆ†é™åºæ’åˆ—
    :param defective_lines: çœŸå®ç¼ºé™·è¡Œçš„é›†åˆ {è¡Œå·1, è¡Œå·2, ...}
    :param k: é€‰å–å‰ k è¡Œ
    :return: 1ï¼ˆåŒ…å«ç¼ºé™·ï¼‰æˆ– 0ï¼ˆä¸åŒ…å«ç¼ºé™·ï¼‰
    """
    sorted_lines = sorted(risk_scores, key=risk_scores.get, reverse=True)[:k]  # å–å‰ k è¡Œ
    return 1 if any(line in defective_lines for line in sorted_lines) else 0


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ----------------- ç¤ºä¾‹ç”¨æ³• -----------------
if __name__ == "__main__":
    # ç¡®ä¿è¿™äº›å‚æ•°ä¸è®­ç»ƒæ—¶ä¸€è‡´
    num_classes = len(torch.unique(torch.stack([data.y for data in all_data])))
    num_relations = all_data[0].edge_attr.size(1) if all_data[0].edge_attr.dim() > 1 else 1
    num_features = all_data[0].x.shape[1]

    # é‡æ–°å®ä¾‹åŒ– RGCN å¹¶åŠ è½½å‚æ•°
    model = RGCN(num_features, 128, num_classes, num_relations)
    model.load_state_dict(torch.load("rgcn_model.pth"))
    model.to(device)
    model.eval()  # è¿›å…¥è¯„ä¼°æ¨¡å¼

    data = torch.load("./data/activemq/activemq-5.0.0/processed/data.pt")  # åŠ è½½æ•°æ®
    num_defective = sum(1 for d in data if d.y.item() == 1)
    num_non_defective = sum(1 for d in data if d.y.item() == 0)

    if isinstance(data, list):
        print(f"ğŸ“Š å‘ç° {len(data)} ä¸ªå›¾æ•°æ®ï¼Œé€ä¸ªåˆ†æ...")
        for i, graph in enumerate(data[:5]):  # éå†å‰ 5 ä¸ªå›¾
            print(f"\nğŸ” åˆ†æç¬¬ {i + 1} ä¸ªå›¾...")
            sorted_risks =analyze_defective_file(model, graph)
    else:
        sorted_risks =analyze_defective_file(model, data)
    risk_scores = {node: score for node, score in sorted_risks}
    for graph in data:  # éå† data åˆ—è¡¨ä¸­çš„æ¯ä¸ª PDG
        defective_lines = {i for i, label in enumerate(graph.node_label) if label == 1}
        total_lines = len(graph.node_label)

        print(f"ğŸ“Œ ä»£ç æ€»è¡Œæ•°: {total_lines}")
        print(f"ğŸ“Œ çœŸå®ç¼ºé™·è¡Œæ•°: {len(defective_lines)}, ç¼ºé™·è¡Œåˆ—è¡¨: {defective_lines}")

    recall = recall_at_top_20_loc(risk_scores, defective_lines, total_lines)
    effort = effort_at_top_20_recall(risk_scores, defective_lines, total_lines)
    ifa = initial_false_alarms(risk_scores, defective_lines)
    top_k_acc = top_k_accuracy(risk_scores, defective_lines, k=10)

    print(f"Recall@Top20%LOC: {recall:.4f}")
    print(f"Effort@Top20%Recall: {effort:.4f}")
    print(f"IFA: {ifa}")
    print(f"Top-k Accuracy: {top_k_acc}")










